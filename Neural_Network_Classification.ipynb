{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network: Classification Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use neural networks to predict the species of iris flowers from data with the following features:\n",
    "\n",
    "    sepal_length - Continuous variable measured in centimeters.\n",
    "    sepal_width - Continuous variable measured in centimeters.\n",
    "    petal_length - Continuous variable measured in centimeters.\n",
    "    petal_width - Continuous variable measured in centimeters.\n",
    "    species - Categorical. 2 species of iris flowers, Iris-virginica or Iris-versicolor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal_length  sepal_width  petal_length  petal_width          species\n",
      "8            6.6          2.9           4.6          1.3  Iris-versicolor\n",
      "39           5.5          2.5           4.0          1.3  Iris-versicolor\n",
      "7            4.9          2.4           3.3          1.0  Iris-versicolor\n",
      "72           7.7          2.8           6.7          2.0   Iris-virginica\n",
      "86           6.3          3.4           5.6          2.4   Iris-virginica\n",
      "['Iris-versicolor' 'Iris-virginica']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd2cc00ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read in dataset\n",
    "iris = pandas.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# shuffle rows\n",
    "shuffled_rows = np.random.permutation(iris.index)\n",
    "iris = iris.loc[shuffled_rows,:]\n",
    "\n",
    "print(iris.head())\n",
    "\n",
    "# There are 2 species\n",
    "print(iris.species.unique())\n",
    "\n",
    "iris.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris[\"ones\"] = np.ones(iris.shape[0])\n",
    "X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = (iris.species == 'Iris-versicolor').values.astype(int)\n",
    "X_train = X[0:70]\n",
    "y_train = y[0:70]\n",
    "X_test = X[70:]\n",
    "y_test= y[70:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale Data\n",
    "\n",
    "You can rescale your data using scikit-learn using the MinMaxScaler class.\n",
    "\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms like:\n",
    "* **gradient descent** \n",
    "* algorithms that weight inputs like **regression** and **neural networks** \n",
    "* algorithms that use distance measures like **K-Nearest Neighbors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train1 = scaler.fit_transform(X_train)\n",
    "X_test1 = scaler.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.56666667  0.5         0.41025641  0.2       ]\n",
      " [ 0.          0.2         0.27777778  0.25641026  0.2       ]\n",
      " [ 0.          0.          0.22222222  0.07692308  0.        ]]\n",
      "[[ 0.          0.47826087  0.          0.57692308  0.33333333]\n",
      " [ 0.          0.60869565  0.21428571  0.57692308  0.6       ]\n",
      " [ 0.          1.          1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train1[:3])\n",
    "print(X_test1[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardisation\n",
    "\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.5125002   0.08457092 -0.37251647 -0.88964745]\n",
      " [ 0.         -1.15539985 -1.12358508 -1.10294091 -0.88964745]\n",
      " [ 0.         -2.06516352 -1.42562408 -1.95510276 -1.59947255]]\n",
      "[[-1.         -8.76972618 -8.6745601  -5.270106   -3.17686165]\n",
      " [-1.         -8.57195147 -8.02733367 -5.270106   -2.54590601]\n",
      " [-1.         -7.97862734 -5.6541701  -4.75506313 -1.59947255]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_train2 = scaler.transform(X_train)\n",
    "X_test2 = scaler.transform(X_test1)\n",
    "\n",
    "\n",
    "print(X_train2[:3])\n",
    "print(X_test2[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalisation\n",
    "\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "\n",
    "This preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as **neural networks** and algorithms that use distance measures such as **K-Nearest Neighbors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11484395  0.75797005  0.33304745  0.52828216  0.14929713]\n",
      " [ 0.13460767  0.74034218  0.33651917  0.53843068  0.17498997]\n",
      " [ 0.15310507  0.75021485  0.36745217  0.50524673  0.15310507]]\n",
      "[[ 0.12030742  0.7218445   0.26467632  0.60153709  0.18046113]\n",
      " [ 0.11504898  0.72480856  0.28762245  0.57524489  0.21859306]\n",
      " [ 0.0956686   0.68881389  0.34440695  0.58357844  0.23917149]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "X_train3 = scaler.transform(X_train)\n",
    "X_test3 = scaler.transform(X_test)\n",
    "\n",
    "print(X_train3[:3])\n",
    "print(X_test3[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize Data\n",
    "\n",
    "It can be useful when you have probabilities that you want to make crisp values. It is also useful when feature engineering and you want to add new features that indicate something meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n",
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5).fit(X_train)\n",
    "\n",
    "X_train4 = binarizer.transform(X_train)\n",
    "X_test4 = binarizer.transform(X_test)\n",
    "\n",
    "print(X_train4[:3])\n",
    "print(X_test4[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Classifier model\n",
    "\n",
    "Let's apply this model to our rescale data. The model is fit on train data then prediction is made from the test data set and repot is made comparing the real output to the predicted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=4, max_iter = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=4, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0]\n",
      "[0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test1)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how good is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0]\n",
      " [ 1 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positive (TP) and true negatives (TN) are the observations that are correctly predicted \n",
    "\n",
    "False Positives (FP) – When actual class is no and predicted class is yes.\n",
    "\n",
    "False Negatives (FN) – When actual class is yes but predicted class in no.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. \n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. \n",
    "\n",
    "Precision = TP/TP+FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "\n",
    "Recall = TP/TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score - F1 Score is the weighted average of Precision and Recall.\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize thetas randomly .\n",
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "def sigmoid_activation(X, theta):\n",
    "    return 1 / (1 + np.exp(-np.dot(X, theta)))\n",
    "                \n",
    "h1 = sigmoid_activation(X_train, theta_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, \n",
    "                   input_layer_size, \n",
    "                   hidden_layer_size, \n",
    "                   num_labels, \n",
    "                   X, y, lambda)\n",
    "# Implements the neural network cost function for a two layer\n",
    "# neural network which performs classification\n",
    "#   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "#   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "#  parameters for the neural network are \"unrolled\" into the vector\n",
    "#   nn_params and need to be converted back into the weight matrices. \n",
    "#\n",
    "#   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "#   partial derivatives of the neural network.\n",
    "#\n",
    "\n",
    "# Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "# for our 2 layer neural network\n",
    "Theta1 = nn_params[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, (input_layer_size + 1))\n",
    "# Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "#                hidden_layer_size, (input_layer_size + 1));\n",
    "Theta2 = nn_params[1 + (hidden_layer_size * (input_layer_size + 1)):].reshape((num_labels, (hidden_layer_size + 1))\n",
    "\n",
    "# Setup some useful variables\n",
    "m = size(X, 1);\n",
    "         \n",
    "# You need to return the following variables correctly \n",
    "J = 0;\n",
    "Theta1_grad = zeros(Theta1.shape);\n",
    "Theta2_grad = zeros(Theta2.shape);\n",
    "                                                                              \n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: You should complete the code by working through the\n",
    "#              following parts.\n",
    "#\n",
    "# Part 1: Feedforward the neural network and return the cost in the\n",
    "#         variable J. After implementing Part 1, you can verify that your\n",
    "#         cost function computation is correct by verifying the cost\n",
    "#         computed in ex4.m\n",
    "#\n",
    "# Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "#         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "#         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "#         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "#         that your implementation is correct by running checkNNGradients\n",
    "#\n",
    "#         Note: The vector y passed into the function is a vector of labels\n",
    "#               containing values from 1..K. You need to map this vector into a \n",
    "#               binary vector of 1's and 0's to be used with the neural network\n",
    "#               cost function.\n",
    "#\n",
    "#         Hint: We recommend implementing backpropagation using a for-loop\n",
    "#               over the training examples if you are implementing it for the \n",
    "#               first time.\n",
    "#\n",
    "# Part 3: Implement regularization with the cost function and gradients.\n",
    "#\n",
    "#         Hint: You can implement this around the code for\n",
    "#               backpropagation. That is, you can compute the gradients for\n",
    "#               the regularization separately and then add them to Theta1_grad\n",
    "#               and Theta2_grad from Part 2.\n",
    "#\n",
    "\n",
    "% Add ones to the X data matrix\n",
    "X = [ones(m, 1) X];\n",
    "D_1 = zeros(hidden_layer_size,input_layer_size+1);\n",
    "D_2 = zeros(num_labels,hidden_layer_size+1);\n",
    "\n",
    "for i=1:m\n",
    "\tindex = y(i);\n",
    "\tyi = zeros(num_labels,1);\n",
    "\tyi(index)=1;\n",
    "\t\n",
    "% a1 est un vecteur vertical (dim a1 = nombre units dans layer 1 + 1 car biais x 1):\n",
    "\ta1 = X(i,:)';\n",
    "\n",
    "\tz2 = Theta1 * a1;\n",
    "\ta2 = sigmoid(z2);\n",
    "\t\n",
    "% Add one to the a vector\n",
    "\ta2 = [1;a2];\n",
    "\n",
    "\ta3 = sigmoid(Theta2 * a2);\n",
    "\n",
    "\tJ = J + sum(-yi .* log(a3) - (1-yi) .* log(1-a3));\n",
    "\n",
    "% delta3 est un vecteur vertical (dim delta_3 = nb units dans layer3 x 1)\n",
    "\tdelta_3 = a3 - yi;\n",
    "% Theta2 est une matrice (dim = nb units dans layer3 x nb units dans layer 2 + 1 car biais- on enleve le biais)\n",
    "% z2 est un vecteur vertical de dimension = nb units dans layer2)\n",
    "\tdelta_2 = Theta2(:,2:end)' * delta_3 .* sigmoidGradient(z2);\n",
    "\n",
    "\n",
    "% D_1 matrice de dim layer2 x layer1 + 1 car biais dans a1\n",
    "\tD_1 = D_1 + delta_2 * a1';\n",
    "\n",
    "% D_2 matrice de dim layer3 x layer2 + 1 car biais dans a2\n",
    "\tD_2 = D_2 + delta_3 * a2';\n",
    "\n",
    "end\n",
    "\n",
    "J=J/m+(sum(sum(Theta1(:,2:end).^2))+sum(sum(Theta2(:,2:end).^2)))*lambda/(2*m);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Theta1_grad = (1/m) * D_1;\n",
    "Theta2_grad = (1/m) * D_2;\n",
    "\n",
    "# Regularisation\n",
    "Theta1_grad(:,2:end) += (lambda / m) * Theta1(:,2:end);\n",
    "Theta2_grad(:,2:end) += (lambda / m) * Theta2(:,2:end);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Unroll gradients\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGradient(J, theta):\n",
    "    '''\n",
    "    ComputeGradient computes the gradient using \"finite differences\"\n",
    "    and gives us a numerical estimate of the gradient.\n",
    "    Computes the numerical gradient of the function J around theta. Calling y = J(theta) should\n",
    "    return the function value at theta.\n",
    "    '''\n",
    "    numgrad = np.empty(shape=theta.shape);\n",
    "    perturb = np.empty(shape=theta.shape);\n",
    "    e = 1E-4;\n",
    "    for p in range(theta.size):\n",
    "        # Set perturbation vector\n",
    "        perturb[p, 0] = e;\n",
    "        loss1 = J(theta - perturb);\n",
    "        loss2 = J(theta + perturb);\n",
    "    # Compute Numerical Gradient\n",
    "        numgrad[p,0] = (loss2 - loss1) / (2*e);\n",
    "        perturb[p, 0] = 0;\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in)\n",
    "    # Initialises the weights of a layer with fan_in\n",
    "    # incoming connections and fan_out outgoing connections using a fixed\n",
    "    # strategy, this will help you later in debugging\n",
    "    # Initialises the weights \n",
    "    #  of a layer with fan_in incoming connections and fan_out outgoing \n",
    "    #   connections using a fix set of values\n",
    "    #\n",
    "    #   Note that W should be set to a matrix of shape(1 + fan_in, fan_out) as\n",
    "    #   the first row of W handles the \"bias\" terms\n",
    "    #\n",
    "\n",
    "    # Initialise W\n",
    "    W = np.empty(shape =(fan_out, 1 + fan_in);\n",
    "\n",
    "    # Initialize W using \"sin\", this ensures that W is always of the same\n",
    "    # values and will be useful for debugging\n",
    "    W = np.sin(range(W.size)) / 10;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, Theta1, Theta2):\n",
    "# Predict the label of an input given a trained neural network\n",
    "# outputs the predicted label of X given the\n",
    "#   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "# Useful values\n",
    "m = X.shape[0]\n",
    "num_labels = Theta2.shape[1];\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "p =  np.empty(shape=(X.shape[0],1));\n",
    "\n",
    "H1 = sigmoid(np.dot(X, Theta1));\n",
    "X2 = np.append(np.ones([m, 1], dtype=np.int32), H1, axis=1)\n",
    "H2 = sigmoid(np.dot(X2, Theta2);\n",
    "# [dummy, p] = max(h2, [], 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-b66aa23e1164>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-b66aa23e1164>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    lambda = 0;\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "\n",
    "# =============== Part 4: Implement Regularization ===============\n",
    "#  Once your cost function implementation is correct, you should now\n",
    "#  continue to implement the regularization with the cost.\n",
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, \n",
    "                   num_labels, X, y, lambda)\n",
    "\n",
    "# ================ Part 5: Sigmoid Gradient  ================\n",
    "#  Before you start implementing the neural network, you will first\n",
    "#  implement the gradient for the sigmoid function. \n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1])\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print({:f}.format(g))\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# ================ Part 6: Initializing Pameters ================\n",
    "# In this part of the exercise, you will be starting to implment a two\n",
    "#  layer neural network that classifies digits. You will start by\n",
    "#  implementing a function to initialize the weights of the neural network\n",
    "#  (randInitializeWeights.m)\n",
    "\n",
    "print('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "\n",
    "\n",
    "# =============== Part 7: Implement Backpropagation ===============\n",
    "#  Once your cost matches up with ours, you should proceed to implement the\n",
    "#  backpropagation algorithm for the neural network. You should add to the\n",
    "#  code you've written in nnCostFunction.m to return the partial\n",
    "#  derivatives of the parameters.\n",
    "\n",
    "print('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "#  Check gradients by running checkNNGradients\n",
    "checkNNGradients()\n",
    "\n",
    "# =============== Part 8: Implement Regularization ===============\n",
    "#  Once your backpropagation implementation is correct, you should now\n",
    "#  continue to implement the regularization with the cost and gradient.\n",
    "#\n",
    "\n",
    "print('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "#  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, \n",
    "                          hidden_layer_size, num_labels, X, y, lambda)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters \\\n",
    "(w/ lambda = {:f}): {:f} \\n(for lambda = 3, \\\n",
    "this value should be about 0.576051)\\n\\n'.format( lambda, debug_J)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================== Part 8: Training NN ===================\n",
    "#  You have now implemented all the code necessary to train a neural \n",
    "#  network. To train your neural network, we will now use \"fmincg\", which\n",
    "#  is a function which works similarly to \"fminunc\". Recall that these\n",
    "#  advanced optimizers are able to train our cost functions efficiently as\n",
    "#  long as we provide them with the gradient computations.\n",
    "#\n",
    "print('\\nTraining Neural Network... \\n')\n",
    "\n",
    "#  After you have completed the assignment, change the MaxIter to a larger\n",
    "#  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = nnCostFunction(p, \n",
    "                              input_layer_size, \n",
    "                              hidden_layer_size, \n",
    "                              num_labels, X, y, lambda)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument (the\n",
    "# neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), \n",
    "                 hidden_layer_size, (input_layer_size + 1))\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "\n",
    "\n",
    "%% ================= Part 9: Visualize Weights =================\n",
    "%  You can now \"visualize\" what the neural network is learning by \n",
    "%  displaying the hidden units to see what features they are capturing in \n",
    "%  the data.\n",
    "\n",
    "fprintf('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1(:, 2:end));\n",
    "\n",
    "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ================= Part 10: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Use a class for this model, it's good practice and condenses the code.\n",
    "class NNet3:\n",
    "    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.maxepochs = int(maxepochs)\n",
    "        self.convergence_thres = 1e-5\n",
    "        self.hidden_layer = int(hidden_layer)\n",
    "        \n",
    "    def _multiplecost(self, X, y):\n",
    "        # Feed through network.\n",
    "        l1, l2 = self._feedforward(X) \n",
    "        # Compute error.\n",
    "        inner = y * np.log(l2) + (1-y) * np.log(1-l2)\n",
    "        # Negative of average error.\n",
    "        return -np.mean(inner)\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        # Feedforward to the first layer.\n",
    "        h = sigmoid_activation(X, self.theta)\n",
    "        # Add a column of ones for bias term.\n",
    "        l1 = np.column_stack([np.ones(l1.shape[0]), l1])\n",
    "        # Activation units are then inputted to the output layer.\n",
    "        l2 = sigmoid_activation(l1.T, self.theta1)\n",
    "        return l1, l2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, y = self._feedforward(X)\n",
    "        return y\n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        nobs, ncols = X.shape\n",
    "        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))\n",
    "        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))\n",
    "        \n",
    "        self.costs = []\n",
    "        cost = self._multiplecost(X, y)\n",
    "        self.costs.append(cost)\n",
    "        costprev = cost + self.convergence_thres+1 # Set an inital costprev to past while loop.\n",
    "        counter = 0 # Intialize a counter.\n",
    "\n",
    "        # Loop through until convergence.\n",
    "        for counter in range(self.maxepochs):\n",
    "            # Feedforward through network.\n",
    "            l1, l2 = self._feedforward(X)\n",
    "\n",
    "            # Start backpropagation.\n",
    "            # Compute gradients.\n",
    "            l2_delta = (y-l2) * l2 * (1-l2)\n",
    "            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)\n",
    "\n",
    "            # Update parameters by averaging gradients and multiplying by the learning rate.\n",
    "            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate\n",
    "            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate\n",
    "            \n",
    "            # Store costs and check for convergence.\n",
    "            counter += 1\n",
    "            costprev = cost # Store prev cost.\n",
    "            cost = self._multiplecost(X, y) # Get next cost.\n",
    "            self.costs.append(cost)\n",
    "            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:\n",
    "                break\n",
    "\n",
    "# Set a learning rate.\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent.\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres.\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units.\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model.\n",
    "model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer=hidden_units)\n",
    "# Train model.\n",
    "model.learn(X, y)\n",
    "\n",
    "# Plot costs.\n",
    "plt.plot(model.costs)\n",
    "plt.title(\"Convergence of the Cost Function\")\n",
    "plt.ylabel(\"J($\\Theta$)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
