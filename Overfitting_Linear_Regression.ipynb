{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting Linear Regression\n",
    "\n",
    "The starter code imports Pandas, reads the data into a Dataframe, and cleans up some messy values. The dataset is hosted by the University of California Irvine on their machine learning repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\", \"car name\"]\n",
    "cars = pd.read_table(\"data/auto-mpg.data\", delim_whitespace=True, names=columns)\n",
    "cars = cars[cars['horsepower'] != '?']\n",
    "cars['horsepower'] = cars['horsepower'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance.\n",
    "\n",
    "In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "* Let's train a linear regression model using:\n",
    "    * The columns in cols as the features,\n",
    "    * The mpg column as the target variable.\n",
    "\n",
    "* Let's use the trained model to make predictions using the same input it was trained on,\n",
    "* Let's compute the variance (TODO) of the predicted values and the mean squared error between the predicted values and the actual label (mpg column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyl_mse: 24.02017956815553 and cyl_var 36.74255887416017\n",
      "weight_mse: 18.6766165974193 and weight_var 42.08612184489641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def train_and_test(cols):\n",
    "    # take in cols: a list of column\n",
    "    # Returns a tuple (MSE, Variance)\n",
    "    X = cars[cols]\n",
    "    y = cars['mpg']\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    y_predict = lr.predict(X)\n",
    "    mse = mean_squared_error(y_predict, y)\n",
    "    variance = np.var(y_predict)\n",
    "    return mse, variance\n",
    "                                \n",
    "                                \n",
    "cyl_mse, cyl_var = train_and_test(['cylinders'])\n",
    "\n",
    "weight_mse, weight_var = train_and_test(['weight'])\n",
    "\n",
    "print('cyl_mse: {} and cyl_var {}'.format(cyl_mse, cyl_var))\n",
    "print('weight_mse: {} and weight_var {}'.format(weight_mse, weight_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_mse, one_var = train_and_test([\"cylinders\"])\n",
    "two_mse, two_var = train_and_test([\"cylinders\", 'displacement'])\n",
    "three_mse, three_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.02017956815553, 36.742558874160167)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_mse, one_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.282057055586364, 39.480681386729316)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_mse, two_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.252954839714231, 40.50978360260148)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_mse, three_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.76139610540622, 43.001342336909389)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_mse, five_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.590170981415227, 49.172567460900481)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_mse, six_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seven_mse, seven_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multivariate regression models you trained got progressively better at reducing the amount of error.\n",
    "\n",
    "A good way to detect if your model is overfitting is to compare the in-sample error and the out-of-sample error, or the training error with the test error. So far, we calculated the in sample error by testing the model over the same data it was trained on. To calculate the out-of-sample error, we need to test the data on a test set of data. We unfortunately don't have a separate test dataset and we'll instead use cross validation.\n",
    "\n",
    "If a model's cross validation error (out-of-sample error) is much higher than the in sample error, then your data science senses should start to tingle. This is the first line of defense against overfitting and is a clear indicator that the trained model doesn't generalize well outside of the training set.\n",
    "\n",
    "Let's create a new function to handle performing the cross validation and computing the cross validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function named train_and_cross_val that:\n",
    "    * takes in a single parameter (list of column names),\n",
    "    * trains a linear regression model using the features specified in the parameter,\n",
    "    * uses the KFold class to perform 10-fold validation using a random seed of 3 (we use this seed to answer check your code),\n",
    "    * calculates the mean squared error across all folds and the mean variance across all folds.\n",
    "    * returns the mean squared error value then the variance using a multiple return statement (e.g. return(avg_mse, avg_var)).\n",
    "* Use the train_and_cross_val function to train linear regression models using the following columns as the features:\n",
    "    * the cylinders and displacement columns. Assign the resulting mean squared error value to two_mse and the resulting variance value to two_var.\n",
    "    * the cylinders, displacement, and horsepower columns. Assign the resulting mean squared error value to three_mse and the resulting variance value to three_var.\n",
    "    * the cylinders, displacement, horsepower, and weight columns. Assign the resulting mean squared error value to four_mse and the resulting variance value to four_var.\n",
    "    * the cylinders, displacement, horsepower, weight, acceleration columns. Assign the resulting mean squared error value to five_mse and the resulting variance value to five_var.\n",
    "    * the cylinders, displacement, horsepower, weight, acceleration, and model year columns. Assign the resulting mean squared error value to six_mse and the resulting variance value to six_var.\n",
    "    * the cylinders, displacement, horsepower, weight, acceleration, model year, and origin columns. Assign the resulting mean squared error value to seven_mse and the resulting variance value to seven_var.\n",
    "    * Use the variable display to inspect each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def train_and_cross_val(cols):\n",
    "    # Takes in a list of column names\n",
    "    # Returns the average MSE and average variance\n",
    "    X = cars[cols]\n",
    "    y = cars[\"mpg\"]\n",
    "    variance_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=3) \n",
    "    \n",
    "    for train_index, test_index in kf.split(X): \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        var = np.var(predictions)\n",
    "        variance_values.append(var)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    avg_mse = np.mean(mse_values)\n",
    "    avg_var = np.mean(variance_values)\n",
    "    return(avg_mse, avg_var)\n",
    "one_mse, one_var = train_and_cross_val([\"cylinders\"])\n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", 'displacement'])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.271251604850704, 35.99007106231538)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_mse, one_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.584370274954374, 38.902525313756023)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_mse, two_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.655622193882955, 40.091287956606941)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_mse, three_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.169683239081884, 42.507643643644386)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_mse, four_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.283038517172052, 42.598736300146825)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_mse, five_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.09968542546712, 48.928246967718017)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_mse, six_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.418131971812052, 49.90431373098729)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_mse, seven_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross validation, the more features we added to the model, the lower the mean squared error got. This is a good sign and indicates that the model generalizes well to new data it wasn't trained on. As the mean squared error value went down, however, the variance of the predictions went up. This is to be expected, since the models with lower squared error values had higher model complexity, which tends to be more sensitive to small variations in input values (or high variance).\n",
    "\n",
    "For each model, let's plot the error and variance to get a better idea of the tradeoff as the number of features increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea90011828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "cols = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"]\n",
    "mse_list = []\n",
    "var_list = []\n",
    "nb_features = len(cols)\n",
    "for i in range(nb_features):\n",
    "    mse, var = train_and_cross_val(cols[:i+1])   \n",
    "    mse_list.append(mse)\n",
    "    var_list.append(var)\n",
    "\n",
    "\n",
    "plt.scatter([2,3,4,5,6,7], mse_list[1:], c='r')\n",
    "plt.scatter([2,3,4,5,6,7], var_list[1:], c='b')\n",
    "plt.xlim(1, 8)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While the higher order multivariate models overfit in relation to the lower order multivariate models, the in-sample error and out-of-sample didn't deviate by much. The best model was around 50% more accurate than the simplest model. On the other hand, the overall variance increased around 25% as we increased the model complexity. The increased variance with the increased model complexity means that your model will have more unpredictable performance on truly new, unseen data.\n",
    "\n",
    "You need to confirm the predictive accuracy of the model using completely new, unobserved data (e.g. maybe from cars from later years). Since often you can't wait until a model is deployed in the wild to know how well it works, the exploration we did in this mission helps you approximate a model's real world performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
